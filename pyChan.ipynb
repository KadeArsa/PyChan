{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('./model/chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "tags = pickle.load(open('./model/tags.pkl','rb'))\n",
    "patterns = pickle.load(open('./model/patterns.pkl','rb'))\n",
    "responses = pickle.load(open('./model/responses.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(inputs, labels):\n",
    "    totalData = len(inputs)\n",
    "    valSize = int(totalData * 0.2)\n",
    "    valSplit = list(range(0, valSize * 5, 5))\n",
    "    trainSplit = [i for i in range(totalData) if i not in valSplit]\n",
    "\n",
    "\n",
    "    trainTexts = [inputs[i] for i in trainSplit]\n",
    "    valTexts = [inputs[i] for i in valSplit]\n",
    "    trainLabels = [labels[i] for i in trainSplit]\n",
    "    valLabels = [labels[i] for i in valSplit]\n",
    "\n",
    "    trainDataset = tf.data.Dataset.from_tensor_slices((trainTexts, trainLabels))\n",
    "    valDataset = tf.data.Dataset.from_tensor_slices((valTexts, valLabels))\n",
    "\n",
    "    return trainDataset, valDataset\n",
    "    \n",
    "trainDataset, valDataset = splitDataset(patterns, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeFunc(sentence):\n",
    "    stopwords = stopWords\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "\n",
    "    for word in stopwords:\n",
    "        if word[0] == \"'\":\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
    "        else:\n",
    "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
    "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLeght = 30\n",
    "def fitVectorizer(trainSentences, standardizeFunc):\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "\t\tstandardize=standardizeFunc,\n",
    "    output_sequence_length=maxLeght\n",
    "\t)\n",
    "    vectorizer.adapt(trainSentences)\n",
    "    return vectorizer\n",
    "\n",
    "textDataset = trainDataset.map(lambda text, label: text)\n",
    "vectorizer = fitVectorizer(textDataset, standardizeFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLabelEncoder(trainLabel, valLabel):\n",
    "    trainLabel = list(trainLabel.as_numpy_iterator())\n",
    "    valLabel = list(valLabel.as_numpy_iterator())\n",
    "    labels = trainLabel + valLabel\n",
    "\n",
    "    labelEncoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
    "    labelEncoder.adapt(labels)\n",
    "\n",
    "    return labelEncoder\n",
    "\n",
    "trainLabels = trainDataset.map(lambda text, label: label)\n",
    "valLabels = valDataset.map(lambda text, label: label)\n",
    "labelEncoder = fitLabelEncoder(trainLabels,valLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Prediksi Masalah :  salam\n",
      "PyChan:  Hai, apakah ada yang bisa saya bantu?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Prediksi Masalah :  Demam\n",
      "PyChan:  Untuk mengobati demam di rumah: 1) Minum banyak cairan agar tetap terhidrasi.2) Berpakaian dengan pakaian ringan.3) Gunakan selimut ringan jika Anda merasa dingin, sampai kedinginan berakhir.4) Ambil acetaminophen (Tylenol, lainnya) atau ibuprofen (Advil, Motrin Ib, lainnya).5) Dapatkan bantuan medis jika demam berlangsung lebih dari lima hari berturut -turut.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyChan: \u001b[39m\u001b[38;5;124m\"\u001b[39m, translator\u001b[38;5;241m.\u001b[39mtranslate(response, src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, dest\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 19\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatbot\u001b[39m():\n\u001b[1;32m---> 19\u001b[0m     inputUser \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     testInput \u001b[38;5;241m=\u001b[39m inputUser\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     21\u001b[0m     predictTag, response, confidence \u001b[38;5;241m=\u001b[39m showRespon(testInput)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:219\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    218\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp)\n\u001b[1;32m--> 219\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# not sure\u001b[39;00m\n\u001b[0;32m    221\u001b[0m should_spacing \u001b[38;5;241m=\u001b[39m parsed[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "def getResponse(predictTag, responses, confidence):\n",
    "    if confidence < 0.6:\n",
    "        return \"Sorry, the input you entered is not clear.\"\n",
    "    return np.random.choice(responses[predictTag])\n",
    "\n",
    "\n",
    "def showRespon(input):\n",
    "    inputA = vectorizer([input])\n",
    "    prediction = model.predict(inputA)\n",
    "\n",
    "    predictClassIndex = np.argmax(prediction, axis=-1)\n",
    "    confidence = prediction[0][predictClassIndex[0]]\n",
    "    predictTag = labelEncoder.get_vocabulary()[predictClassIndex[0]]\n",
    "    response = getResponse(predictTag, responses, confidence)\n",
    "\n",
    "    return predictTag, response, confidence\n",
    "\n",
    "def chatbot():\n",
    "    inputUser = translator.translate(input(\"User: \"), src=\"id\", dest=\"en\")\n",
    "    testInput = inputUser.text\n",
    "    predictTag, response, confidence = showRespon(testInput)\n",
    "    print(f\"Prediksi Masalah : \", translator.translate(predictTag, src=\"en\", dest=\"id\").text)\n",
    "    print(f\"PyChan: \", translator.translate(response, src=\"en\", dest=\"id\").text)\n",
    "\n",
    "while True:\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
